---
title: "Метод наименьших квадратов в задаче линейной и нелинейной регрессии. Непараметрические коэффициенты корреляции. Значимость частных коэффициентов регрессии."
author: "Вейбер Е.Н. 23.М08-мм"
date: "30-04-2024"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Введение

Это отчет о моделировании нелинейной модели y = f(x, a, b) + δ с несмещенной нормально распределенной ошибкой, дисперсия которой равна ε, считая x стандартно нормально распределенной случайной величиной. Вариант 18.

## Модель была промоделирована следующим кодом:
```{r}
#Моделирование нелинейной модели
# Загрузка необходимых библиотек
library(nlstools)
library(ggplot2)
library(tidyverse)
library(readr)
library(corrplot)
library(tidyr)
library(GGally)
library(plotly)
library(reshape2)

# Установка начальных параметров
a <- 0.14
b <- 1
epsilon <- 0.33

# Генерация данных
set.seed(123)  # для воспроизводимости
x <- seq(0, 10, length.out = 100)
y_true <- exp(a * x + b)
y <- y_true + rnorm(length(x), mean = 0, sd = epsilon)

# Моделирование данных
nl_model <- nls(y ~ exp(a * x + b),
             start = list(a = 0.1, b = 0.9),
             algorithm = "port",
             control = nls.control(maxiter = 100))

# Вывод результатов моделирования
print(summary(nl_model))

# Визуализация результатов
ggplot(data = data.frame(x, y, y_fitted = predict(nl_model)), aes(x = x)) +
  geom_point(aes(y = y), color = 'blue') +
  geom_line(aes(y = y_fitted), color = 'red') +
  ggtitle("Нелинейная модель: y = exp(a*x + b)") +
  theme_minimal()
```

**Анализ параметров:**
  - **Параметр a:** Оценка параметра `a` составляет 0.141328, со стандартной ошибкой 0.001923. Значение t-статистики для этого коэффициента составляет 73.49, что указывает на его статистическую значимость (p-value < 2e-16). Это означает, что с очень высокой степенью уверенности коэффициент `a` значимо отличается от нуля, и его влияние на зависимую переменную `y` значимо в модели.   
  - **Параметр b:** Оценка параметра `b` равна 0.996550 с соответствующей стандартной ошибкой 0.014485. Значение t-статистики для `b` равно 68.80, с p-value < 2e-16, что также подтверждает его статистическую значимость. Это означает, что параметр `b` также оказывает значимое влияние на зависимую переменную.   

**Стандартная ошибка остатков:**   
  - Стандартная ошибка остатков составляет 0.301. Это показатель точности модели: меньшие значения указывают на более высокую точность модели в предсказании данных.   

**Степени свободы:**   
  - Модель имеет 98 степеней свободы остатков, что достаточно для надёжной оценки статистической значимости параметров.   

**Алгоритм и сходимость:**   
  - Используемый алгоритм "port" сообщает о том, что модель сходится (сообщение о сходимости: "relative convergence (4)"). Это означает, что итерационный процесс оптимизации нашёл решение, удовлетворяющее заданным критериям сходимости.   

Эти результаты показывают, что модель адекватно описывает зависимость между `x` и `y`, и оба параметра модели значимо влияют на результат.   

### Построение линейной модели с теми же параметрами
```{r}
# Моделирование линейной модели
lm_model <- lm(y ~ x)
# Вывод результатов моделирования
print(summary(lm_model))

# Визуализация результатов
data_frame <- data.frame(x, y, y_fitted_nl = predict(nl_model), y_fitted_lm = predict(lm_model))
ggplot(data = data_frame, aes(x = x)) +
  geom_point(aes(y = y), color = 'blue', alpha = 0.5) +
  geom_line(aes(y = y_fitted_nl), color = 'red') +
  geom_line(aes(y = y_fitted_lm), color = 'green', linetype = "dashed") +
  ggtitle("Сравнение моделей: Нелинейная (Красная) vs Линейная (Зелёная)") +
  theme_minimal()
```

**Анализ остатков:**   
- Распределение остатков варьируется от -1.08124 до 1.49452 с медианой, близкой к нулю (-0.05955). Это указывает на то, что остатки центрированы вокруг нуля, что является хорошим признаком адекватности модели.   

**Коэффициенты:**   
  - **Пересечение (Intercept)**: Оценка составляет 1.90363 с стандартной ошибкой 0.10931. Статистическая значимость этого коэффициента очень высока (p-value < 2e-16), что означает, что при \( x = 0 \), среднее значение \( y \) значимо отличается от 0.   
  - **Наклон (x)**: Коэффициент при \( x \) равен 0.81355 со стандартной ошибкой 0.01888 и очень высокой t-статистикой (43.08). Это указывает на сильную и значимую связь между \( x \) и \( y \).   

**Статистика модели:**   
  - **Стандартная ошибка остатков (Residual standard error)**: Значение 0.5506 на 98 степенях свободы показывает, насколько велики типичные остатки.   
  - **R-квадрат (Multiple R-squared)**: Значение 0.9498 означает, что примерно 94.98% вариации зависимой переменной \( y \) объясняется моделью. Это очень высокий показатель, указывающий на хорошее качество модели.   
  - **Скорректированный R-квадрат (Adjusted R-squared)**: Значение 0.9493, почти такое же, как и R-квадрат, что также подтверждает эффективность модели.   
  - **F-статистика**: Значение 1856 на 1 и 98 степенях свободы с p-value < 2.2e-16 подтверждает, что модель статистически значимо лучше модели без предикторов (только с константой).   

Эти результаты свидетельствуют о том, что линейная регрессионная модель хорошо подходит для анализируемых данных, и взаимосвязь между переменными значима и выражена.  

На представленном графике мы видим сравнение двух моделей: нелинейной (красная линия) и линейной (зелёная пунктирная линия) на одних и тех же данных.  

1. **Нелинейная модель (Красная линия):** Эта модель следует экспоненциальной функции. Она хорошо описывает тренд данных, плавно и точно следуя изменениям в значениях переменной \( y \). Видно, что красная линия проходит через середину большинства точек данных, что свидетельствует о хорошем соответствии модели данным.   

2. **Линейная модель (Зелёная пунктирная линия):** Эта модель представляет собой простую линейную регрессию. Она показывает общий тренд данных, однако не улавливает более сложные закономерности в изменении переменной \( y \), как это делает нелинейная модель. Линейная модель кажется менее подходящей для данных, так как она не отражает криволинейный характер тренда, который явно присутствует в данных.   

**Заключение:**   
Нелинейная модель лучше подходит для данных, представленных на графике, так как она точнее отображает зависимости между переменными, улавливая нелинейный тренд. Это подтверждается тем, что красная линия лучше соответствует распределению точек, чем зелёная пунктирная линия. Если цель анализа — предсказывать или понимать динамику переменной \( y \) в зависимости от \( x \), то использование нелинейной модели будет предпочтительнее.   

### Работа с линейной моделью
```{r}
# Загрузка необходимых библиотек
library(stats)

# lm_model уже определена как lm_model <- lm(y ~ x)

# Выполнение дисперсионного анализа
anova_result <- anova(lm_model)
print(anova_result)

# Вывод сводки модели, включая значимость коэффициентов
summary_result <- summary(lm_model)
print(summary_result)

# Проверка значимости прогноза (общей модели)
print("Проверка значимости прогноза (F-статистика):")
print(summary_result$fstatistic)

# Проверка значимости коэффициентов
print("Проверка значимости коэффициентов регрессии:")
print(coef(summary_result))

# Для сравнения вручную вычислить F-статистику и p-value
rss <- sum(residuals(lm_model)^2) # Сумма квадратов остатков
tss <- sum((y - mean(y))^2) # Общая сумма квадратов
df_res <- lm_model$df.residual # Число степеней свободы остатков
df_total <- length(y) - 1 # Общее число степеней свободы
f_statistic <- ((tss - rss) / (df_total - df_res)) / (rss / df_res) # F-статистика
p_value <- pf(f_statistic, df_total - df_res, df_res, lower.tail = FALSE) # P-value для F-статистики

cat(sprintf("Вручную вычисленная F-статистика: %f, P-value: %f\n", f_statistic, p_value))
```
Результаты дисперсионного анализа для модели линейной регрессии с одним предиктором \( x \) и зависимой переменной \( y \):  

**Таблица анализа дисперсии (ANOVA):**   
  - **Степени свободы (Df)**: Переменная \( x \) имеет 1 степень свободы, что указывает на один предиктор в модели. Остаточная компонента (Residuals) имеет 98 степеней свободы, что соответствует числу наблюдений минус количество оцениваемых параметров (100 наблюдений минус 2 параметра: пересечение и коэффициент \( x \)).   
  - **Сумма квадратов (Sum Sq)**: Сумма квадратов, объясненная предиктором \( x \), составляет 562.70, а сумма квадратов остаточных значений (необъясненная моделью) — 29.71.   
  - **Средний квадрат (Mean Sq)**: Средний квадрат для \( x \) равен 562.7, а для остатков — 0.3. Средний квадрат — это сумма квадратов, деленная на соответствующие степени свободы.   
  - **F-значение**: F-статистика составляет 1855.8, что означает отношение среднего квадрата, объясненного моделью, к среднему квадрату остаточной вариации. Это значение указывает на то, что модель значительно лучше модели без предикторов (только с константой).   
  - **P-значение (Pr(>F))**: P-значение меньше 2.2e-16, что свидетельствует о том, что влияние переменной \( x \) на \( y \) статистически значимо на очень низком уровне значимости. Это подтверждает, что предиктор \( x \) имеет существенное влияние на зависимую переменную \( y \).   

**Интерпретация**:   
Эти результаты подтверждают значимость переменной \( x \) в объяснении вариативности \( y \). Высокое значение F-статистики и очень низкое p-значение свидетельствуют о том, что модель адекватно описывает зависимость между переменными, и вклад \( x \) в эту зависимость значим.   

### Проверка значимости прогноза и коэффициентов регрессии
```{r}
# Вывод сводки модели, включая значимость коэффициентов
summary_result <- summary(lm_model)
print(summary_result)

# Проверка значимости прогноза (общей модели)
print("Проверка значимости прогноза (F-статистика):")
print(summary_result$fstatistic)

# Проверка значимости коэффициентов
print("Проверка значимости коэффициентов регрессии:")
print(coef(summary_result))

# Для сравнения можно вручную вычислить F-статистику и p-value
rss <- sum(residuals(lm_model)^2) # Сумма квадратов остатков
tss <- sum((y - mean(y))^2) # Общая сумма квадратов
df_res <- lm_model$df.residual # Число степеней свободы остатков
df_total <- length(y) - 1 # Общее число степеней свободы
f_statistic <- ((tss - rss) / (df_total - df_res)) / (rss / df_res) # F-статистика
p_value <- pf(f_statistic, df_total - df_res, df_res, lower.tail = FALSE) # P-value для F-статистики

cat(sprintf("Вручную вычисленная F-статистика: %f, P-value: %f\n", f_statistic, p_value))
```

**Проверка значимости прогноза (F-статистика):**   
  - **F-значение**: 1855.834 — это квадрат t-значения для наклона (коэффициента при \( x \)), что подтверждает, что модель в целом имеет статистическую значимость. Это значение указывает на то, что модель значимо лучше, чем модель, которая не имеет никаких предикторов (только константа).   
  - **Степени свободы для числителя (numdf)**: 1, что соответствует одному предиктору.   
  - **Степени свободы для знаменателя (dendf)**: 98, что соответствует количеству наблюдений минус количество оцениваемых параметров.   

**Проверка значимости коэффициентов регрессии:**   
- **(Intercept)**:   
  - **Оценка (Estimate)**: 1.90363 — это оценка пересечения.   
  - **Стандартная ошибка (Std. Error)**: 0.10931.   
  - **t-значение**: 17.41542 — это t-статистика, которая показывает, насколько велико влияние пересечения по сравнению с его стандартной ошибкой.   
  - **P-значение (Pr(>|t|))**: 9.22e-32 — статистически значимо, что подтверждает влияние пересечения на зависимую переменную.   
  
- **x**:   
  - **Оценка (Estimate)**: 0.81355 — это оценка наклона.   
  - **Стандартная ошибка (Std. Error)**: 0.01888.   
  - **t-значение**: 43.07940 — указывает на значимость коэффициента при переменной \( x \).   
  - **P-значение (Pr(>|t|))**: 1.71e-65 — значительно меньше стандартного порога значимости, что подтверждает значимость \( x \) в модели.   

**Вывод:**   
Модель значима, как в целом, так и каждый из коэффициентов в отдельности. Переменная \( x \) значимо влияет на \( y \), и интерцепт также статистически значим. Это подтверждается как значениями F-статистики, так и значениями p-статистики для каждого коэффициента. Несмотря на разницу в p-значениях, вычисленных встроенной функцикцией и вручную, оба значения указывают на статистическую значимость модели.   

### Построение корреляционной матрицы и двумерной диаграммы
```{r}
# Загрузка данных
data <- read_delim("~/Downloads/addicts.csv", delim = ";")
head(data)

# Подготовка данных
my_data <- data[ , c("rubsex", "tlfba2", "asi3_alc", "sstati", "tlfbh2")]

# Преобразование из <chr> с запятой в <dbl>
# Преобразование всех колонок в data
for (i in names(my_data)) {
  if (is.character(my_data[[i]])) {
    my_data[[i]] <- gsub(" ", "", my_data[[i]])  # Удаляем пробелы
    my_data[[i]] <- gsub(",", ".", my_data[[i]])  # Заменяем запятые на точки
    my_data[[i]] <- as.numeric(my_data[[i]])     # Преобразование в числовой тип
  }
}

# Вычисление корреляционной матрицы
cor_matrix <- cor(my_data, use = "complete.obs")  # Использование только полных наблюдений
print("Корреляционная матрица:")
print(cor_matrix)

# Создание графика
ggplot(data, aes_string(x = my_data$tlfba2, y = my_data$asi3_alc)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", col = "blue") +  # Линейная модель
  labs(title = paste("Двумерная диаграмма для tlfba2 и asi3_alc"),
       x = my_data$tlfba2, y = my_data$asi3_alc) +
  theme_minimal()
```

Рассмотрим коэффициенты корреляции для датафрейма:   
  - **rubsex** и **tlfbh2** имеют корреляцию 0.12061439, что указывает на небольшую положительную связь между этими переменными.   
  - **tlfba2** и **asi3_alc** имеют корреляцию 0.27536343, это самая сильная положительная корреляция в датафрейме, что может указывать на наличие статистически значимой связи между этими переменными.   
  - Остальные корреляции довольно низкие, что говорит о слабой или отсутствующей линейной связи между соответствующими переменными.   

Наименьшая корреляция наблюдается между **sstati** и **tlfbh2** (0.00471796), что указывает на почти полное отсутствие линейной зависимости между этими переменными.   

На этой двумерной диаграмме показана зависимость между переменной **tlfbh2** и переменной **asi3_alc**. Линия тренда (синяя линия) и серая область, обозначающая 95% доверительный интервал, указывают на положительную корреляцию между этими двумя переменными.   

### Основные наблюдения:   
1. **Корреляция**: Существует положительная линейная зависимость между **asi3_alc** и **tlfbh2**. По мере увеличения значения **asi3_alc**, наблюдается увеличение значений **tlfbh2**.   
2. **Выбросы**: На графике присутствуют значительные выбросы, особенно на верхнем уровне значения **asi3_alc**, что может влиять на устойчивость и точность модели.   
3. **Доверительный интервал**: Широкий доверительный интервал говорит о большом разбросе значений и возможной нестабильности оценок в модели. Это особенно заметно на правом краю графика, где интервал расширяется.   

Эта диаграмма позволяет визуализировать общую тенденцию и вариативность данных, что важно для оценки подходящей статистической модели и понимания возможных ограничений текущего анализа.   

### Построение модели множественной регрессии
```{r}
# Построение модели множественной регрессии
my_model <- lm(tlfbh2 ~ rubsex + tlfba2 + asi3_alc + sstati, data = my_data)

# Вывод результатов модели
summary(my_model)
print(my_model)
```
Результаты анализа модели множественной линейной регрессии, описывающей зависимость переменной **tlfbh2** от переменных **rubsex**, **tlfba2**, **asi3_alc** и **sstati**, приведены ниже:   

### Основные выводы:   

1. **Коэффициенты:**   
   - **(Intercept)** Пересечение с Y-осью при всех независимых переменных, равных нулю, составляет 393.9885 с p-значением 0.0373, что указывает на его статистическую значимость.   
   - **rubsex** Каждое увеличение на единицу в `rubsex` увеличивает `tlfbh2` на 29.6876, p-значение 0.0438, что также является статистически значимым.   
   - **tlfba2** Прирост в `tlfba2` увеличивает `tlfbh2` на 1.0875, однако это изменение не является статистически значимым (p-значение 0.3333).   
   - **asi3_alc** и **sstati** Изменения в этих переменных не оказывают значимого влияния на `tlfbh2` (p-значения 0.9625 и 0.9079 соответственно).   

2. **Качество модели:**   
   - **Residual standard error**: Стандартная ошибка остатков составляет 537.8, что относительно высоко, указывая на значительную остаточную вариативность, которую модель не смогла объяснить.   
   - **Multiple R-squared**: Объясненная моделью дисперсия составляет всего 1.834%, что указывает на низкую объясняющую способность модели.   
   - **Adjusted R-squared**: Скорректированный коэффициент детерминации даже ниже, 0.395%, что подтверждает, что добавление переменных не приводит к значительному улучшению модели.   
   - **F-statistic**: Статистика F-теста равна 1.275 с p-значением 0.2801, что указывает на то, что в целом модель не является статистически значимой.   

### Нахождение частного коэффициента корреляции
```{r}
library(ppcor)

# Функция расчета частного коэффициента корреляции и проверки его значимости
calculate_partial_correlation <- function(data, x, y, controls) {
  # Подготовка данных, удаление NA
  data <- data[, c(x, y, controls)]
  data <- na.omit(data)
  
  # Моделирование зависимой переменной
  formula_y <- reformulate(controls, response = y)
  model_y <- lm(formula_y, data = data)
  residuals_y <- residuals(model_y)
  
  # Моделирование независимой переменной
  formula_x <- reformulate(controls, response = x)
  model_x <- lm(formula_x, data = data)
  residuals_x <- residuals(model_x)
  
  # Расчет корреляции между остатками
  cor_test <- cor.test(residuals_x, residuals_y)
  return(list(correlation = cor_test$estimate, p_value = cor_test$p.value))
}

# Переменные для анализа
features <- c("rubsex", "tlfba2", "asi3_alc", "sstati") # Пример переменных
target <- "tlfbh2" # Зависимая переменная

# Расчет частных корреляций
partial_corrs <- list()
for (var in features) {
  controls <- setdiff(features, var)
  result <- calculate_partial_correlation(my_data, var, target, controls)
  partial_corrs[[var]] <- result
}

# Вывод результатов
for (var in names(partial_corrs)) {
  coef <- partial_corrs[[var]]$correlation
  p_value <- partial_corrs[[var]]$p_value
  cat(sprintf("Признак: %s,\t коэффициент корреляции: %.4f,\t p-value (значимость): %.4f\n", var, coef, p_value))
}
```
Эти результаты указывают на следующее:

1. **rubsex**: С коэффициентом корреляции 0.1216 и p-значением 0.0427, эта переменная показывает статистически значимую слабую положительную связь с зависимой переменной `tlfbh2`. Это означает, что при увеличении `rubsex` на единицу, `tlfbh2` также имеет тенденцию увеличиваться, хотя и не очень сильно.

2. **tlfba2**: С коэффициентом корреляции 0.0586 и p-значением 0.3307, связь между `tlfba2` и `tlfbh2` слабая и статистически не значима. Это говорит о том, что изменения в `tlfba2` не влияют на `tlfbh2` в статистически значимой степени.

3. **asi3_alc**: С очень низким коэффициентом корреляции 0.0028 и p-значением 0.9623, влияние `asi3_alc` на `tlfbh2` практически нулевое и статистически не значимо.

4. **sstati**: Отрицательный коэффициент корреляции -0.0070 и высокое p-значение 0.9074 указывают на отсутствие значимой связи между `sstati` и `tlfbh2`.

Из этих результатов можно сделать вывод, что только переменная `rubsex` имеет значимое влияние на `tlfbh2`. Это важно учитывать при моделировании и анализе данных, где `rubsex` может быть рассмотрен как потенциальный фактор, влияющий на изменения в `tlfbh2`. Остальные переменные, вероятно, не имеют значимого вклада в изменения этой зависимой переменной в данном наборе данных.